{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05655a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import f1_score, hamming_loss, jaccard_score\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LANGUAGE = \"es\"\n",
    "BASE_PATH = \"XED/processed\"\n",
    "MODEL_NAME = \"tiiuae/Falcon3-3B-Instruct\"\n",
    "OUTPUT_DIR_ROOT = f\"./{LANGUAGE}_gpt_results\"\n",
    "LOG_DIR_ROOT = f\"./logs_falcon_{LANGUAGE}\"\n",
    "WEIGHTS_DIR_ROOT = f\"./weights/falcon\"\n",
    "SAVE_DIR_FINAL = os.path.join(\n",
    "    WEIGHTS_DIR_ROOT, f\"lora_falcon_{LANGUAGE}_best_final\"\n",
    ")\n",
    "\n",
    "PARAMS = {\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.10\n",
    "}\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"up_proj\", \"down_proj\", \"gate_proj\"\n",
    "]\n",
    "ALL_LABELS = [str(i) for i in range(1, 9)]\n",
    "MAX_INPUT_LENGTH = 256\n",
    "EVAL_SAMPLES_COUNT_CV = 300\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "\n",
    "def predict_emotions_es(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    df: pd.DataFrame,\n",
    "    max_samples: Optional[int] = 300\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Generate predictions for emotion classification using the Spanish prompt.\"\"\"\n",
    "\n",
    "    preds: List[str] = []\n",
    "    golds: List[str] = []\n",
    "\n",
    "    df_to_process = df.head(max_samples) if max_samples is not None else df\n",
    "    total_samples = len(df_to_process)\n",
    "\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "\n",
    "    for _, row in tqdm(\n",
    "        df_to_process.iterrows(),\n",
    "        total=total_samples,\n",
    "        desc=\"Generating Predictions\"\n",
    "    ):\n",
    "        prompt = f\"Clasifica la emoción en esta frase: {row['text']}\\nEmoción:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        if \"token_type_ids\" in inputs:\n",
    "            inputs.pop(\"token_type_ids\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "            )\n",
    "\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        pred_text = tokenizer.decode(\n",
    "            output[0][input_length:], skip_special_tokens=True\n",
    "        ).strip().lower()\n",
    "\n",
    "        preds.append(pred_text)\n",
    "        golds.append(str(row[\"labels\"]))\n",
    "\n",
    "    return preds, golds\n",
    "\n",
    "\n",
    "def compute_metrics_numeric(\n",
    "    preds: List[str], golds: List[str]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Compute standard classification metrics for multilabel classification.\"\"\"\n",
    "\n",
    "    y_true = np.zeros((len(golds), len(ALL_LABELS)))\n",
    "    y_pred = np.zeros((len(golds), len(ALL_LABELS)))\n",
    "\n",
    "    for i, (g, p) in enumerate(zip(golds, preds)):\n",
    "        true_ids = [s.strip() for s in str(g).split(\",\") if s.strip().isdigit()]\n",
    "        pred_ids = re.findall(r'\\b[1-8]\\b', str(p))\n",
    "\n",
    "        for t in true_ids:\n",
    "            if t in ALL_LABELS:\n",
    "                y_true[i, ALL_LABELS.index(t)] = 1\n",
    "        for t in pred_ids:\n",
    "            if t in ALL_LABELS:\n",
    "                y_pred[i, ALL_LABELS.index(t)] = 1\n",
    "\n",
    "    metrics = {\n",
    "        \"micro_f1\": f1_score(\n",
    "            y_true, y_pred, average=\"micro\", zero_division=0\n",
    "        ),\n",
    "        \"macro_f1\": f1_score(\n",
    "            y_true, y_pred, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"jaccard\": jaccard_score(\n",
    "            y_true, y_pred, average=\"samples\", zero_division=0\n",
    "        ),\n",
    "        \"hamming\": hamming_loss(y_true, y_pred),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def preprocess_function_es(\n",
    "    examples: Dict[str, List[Any]], tokenizer: AutoTokenizer\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Crea muestras de entrenamiento con instrucciones y etiquetas de emoción.\"\"\"\n",
    "\n",
    "    texts = [\n",
    "        f\"Clasifica la emoción en esta frase: {t}\\nEmoción: {l}\"\n",
    "        for t, l in zip(examples[\"text\"], examples[\"labels\"])\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Executes the LoRA fine-tuning and evaluation pipeline.\"\"\"\n",
    "\n",
    "    print(f\"--- Loading Spanish Data (lang: {LANGUAGE}) ---\")\n",
    "    try:\n",
    "        train_df = pd.read_csv(os.path.join(BASE_PATH, f\"train_{LANGUAGE}.csv\"))\n",
    "        test_df = pd.read_csv(os.path.join(BASE_PATH, f\"test_{LANGUAGE}.csv\"))\n",
    "        print(f\"Train samples: {len(train_df)}\")\n",
    "        print(f\"Test samples: {len(test_df)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"Error: Could not find data files for language '{LANGUAGE}'. Exiting.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side='left')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    os.makedirs(os.path.dirname(SAVE_DIR_FINAL), exist_ok=True)\n",
    "\n",
    "    print(\"--- Tokenizing Data ---\")\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    num_proc = os.cpu_count() or 1\n",
    "    preprocess_func = lambda x: preprocess_function_es(x, tokenizer)\n",
    "\n",
    "    tokenized_train = train_dataset.map(\n",
    "        preprocess_func,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        num_proc=num_proc\n",
    "    )\n",
    "    tokenized_test = test_dataset.map(\n",
    "        preprocess_func,\n",
    "        batched=True,\n",
    "        remove_columns=test_dataset.column_names,\n",
    "        num_proc=num_proc\n",
    "    )\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    print(f\"\\n--- Running Zero-Shot Baseline Evaluation on {MODEL_NAME} ---\")\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None\n",
    "    )\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "    base_model.to(device)\n",
    "    base_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    base_preds, base_golds = predict_emotions_es(\n",
    "        base_model, tokenizer, test_df, max_samples=EVAL_SAMPLES_COUNT_CV\n",
    "    )\n",
    "    base_metrics = compute_metrics_numeric(base_preds, base_golds)\n",
    "    print(f\"Base Model Metrics ({MODEL_NAME}):\")\n",
    "    print(base_metrics)\n",
    "\n",
    "    del base_model\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\n--- Setting up LoRA Fine-Tuning with {MODEL_NAME} ---\")\n",
    "\n",
    "    base_model_ft = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None\n",
    "    )\n",
    "    base_model_ft.resize_token_embeddings(len(tokenizer))\n",
    "    base_model_ft.to(device)\n",
    "    base_model_ft.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=PARAMS[\"r\"],\n",
    "        lora_alpha=PARAMS[\"lora_alpha\"],\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        lora_dropout=PARAMS[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(base_model_ft, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    output_dir = os.path.join(\n",
    "        OUTPUT_DIR_ROOT, f\"final_lora_falcon_{LANGUAGE}_best\"\n",
    "    )\n",
    "    logging_dir = os.path.join(LOG_DIR_ROOT, \"final\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=5,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=logging_dir,\n",
    "        report_to=\"none\",\n",
    "        fp16=True if torch.cuda.is_available() and device.type != 'cpu' else False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Starting LoRA Fine-Tuning with {MODEL_NAME} (Spanish) ---\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"\\n--- Running Final Fine-Tuned Model Evaluation ---\")\n",
    "\n",
    "    preds, golds = predict_emotions_es(\n",
    "        model, tokenizer, test_df, max_samples=EVAL_SAMPLES_COUNT_CV\n",
    "    )\n",
    "    metrics = compute_metrics_numeric(preds, golds)\n",
    "    print(f\"Fine-Tuned Model Metrics ({MODEL_NAME} - Final):\")\n",
    "    print(metrics)\n",
    "\n",
    "    os.makedirs(os.path.dirname(SAVE_DIR_FINAL), exist_ok=True)\n",
    "    model.save_pretrained(SAVE_DIR_FINAL)\n",
    "    tokenizer.save_pretrained(SAVE_DIR_FINAL)\n",
    "    print(f\"\\nFinal fine-tuned model saved to {SAVE_DIR_FINAL}\")\n",
    "\n",
    "    del model, base_model_ft, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
