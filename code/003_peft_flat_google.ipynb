{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b877df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, \n",
    "    TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from sklearn.metrics import f1_score, jaccard_score, hamming_loss\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "BASE_PATH = \"XED/processed\"\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "LANGUAGES = [\"en\", \"fi\", \"fr\", \"es\"]\n",
    "SAVE_ROOT_DIR = \"./weights\"\n",
    "RESULTS_FILE = \"./results_lora_flan_t5_base_5_lang.csv\"\n",
    "TRAIN_ARGS_ROOT_DIR = \"./results_lora\"\n",
    "CV_RESULTS_DIR = \"./cv_results\"\n",
    "CV_WEIGHTS_DIR = \"./cv_weights\"\n",
    "\n",
    "MAX_SAMPLES_EVAL = 500\n",
    "CV_EPOCHS = 2  \n",
    "FINAL_EPOCHS = 5 \n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "CV_LANG = \"en\"\n",
    "LORA_GRID = {\n",
    "    \"r\": [8, 16, 32],\n",
    "    \"lora_alpha\": [16, 32, 64]\n",
    "}\n",
    "\n",
    "# Mapping for metric calculation (IDs 1-8 are typically used for emotions)\n",
    "ALL_LABELS = [str(i) for i in range(1, 9)]\n",
    "\n",
    "# --- Model and Tokenizer Initialization ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "BASE_MODEL_CONFIG = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).config\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_data(lang: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load train and test dataframes for a specific language.\"\"\"\n",
    "    train_path = os.path.join(BASE_PATH, f\"train_{lang}.csv\")\n",
    "    test_path = os.path.join(BASE_PATH, f\"test_{lang}.csv\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    return train_df, test_df\n",
    "\n",
    "def preprocess_function(examples: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tokenizes text and labels, preparing data for Seq2Seq training.\n",
    "    \"\"\"\n",
    "    labels_list = [str(l) for l in examples.get(\"labels\", [])]\n",
    "    \n",
    "    texts = [f\"emotion classification: {t}\" for t in examples[\"text\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(texts, max_length=256, truncation=True)\n",
    "    \n",
    "    labels = tokenizer(labels_list, max_length=64, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics_numeric(preds: List[str], golds: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute F1/Jaccard/Hamming metrics by converting label strings \n",
    "    (e.g., '1,4') to binary indicator matrices.\n",
    "    \"\"\"\n",
    "    y_true = np.zeros((len(golds), len(ALL_LABELS)))\n",
    "    y_pred = np.zeros((len(golds), len(ALL_LABELS)))\n",
    "\n",
    "    for i, (g, p) in enumerate(zip(golds, preds)):\n",
    "        # Extract and clean numeric IDs from gold string\n",
    "        true_ids = [s.strip() for s in str(g).split(\",\") if s.strip().isdigit()]\n",
    "        \n",
    "        # Extract numeric IDs robustly from predicted text (LLM output)\n",
    "        pred_ids = re.findall(r'\\d+', str(p)) \n",
    "        \n",
    "        # Populate true matrix\n",
    "        for t in true_ids:\n",
    "            if t in ALL_LABELS:\n",
    "                y_true[i, ALL_LABELS.index(t)] = 1\n",
    "        \n",
    "        # Populate predicted matrix\n",
    "        for t in pred_ids:\n",
    "            if t in ALL_LABELS:\n",
    "                y_pred[i, ALL_LABELS.index(t)] = 1\n",
    "\n",
    "    metrics = {\n",
    "        \"micro_f1\": f1_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"jaccard\": jaccard_score(y_true, y_pred, average=\"samples\", zero_division=0),\n",
    "        \"hamming\": hamming_loss(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def predict_emotions_llm(\n",
    "        model: PeftModel, \n",
    "        tokenizer: AutoTokenizer, \n",
    "        df: pd.DataFrame, \n",
    "        lang: str\n",
    "        ) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Generate model predictions using the current model (LoRA adapter) for evaluation.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    preds: List[str] = []\n",
    "    golds: List[str] = []\n",
    "    \n",
    "    df_subset = df.head(MAX_SAMPLES_EVAL)\n",
    "\n",
    "    device = model.device\n",
    "    \n",
    "    for _, row in tqdm(df_subset.iterrows(), total=len(df_subset), desc=f\"Evaluating {lang}\"):\n",
    "        text = f\"emotion classification: {row['text']}\" \n",
    "        \n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, max_new_tokens=20)\n",
    "            \n",
    "        pred_text = tokenizer.decode(output[0], skip_special_tokens=True).lower()\n",
    "        preds.append(pred_text)\n",
    "        golds.append(str(row[\"labels\"]))\n",
    "\n",
    "    return preds, golds\n",
    "\n",
    "def run_peft_training_and_evaluation(\n",
    "    lang: str,\n",
    "    peft_model: PeftModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    training_data: pd.DataFrame,\n",
    "    test_data: pd.DataFrame,\n",
    "    epochs: int,\n",
    "    is_multilingual: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Performs LoRA fine-tuning for a single language split and evaluates it.\n",
    "    \n",
    "    Returns a dictionary of metrics.\n",
    "    \"\"\"\n",
    "    lang_tag = lang if not is_multilingual else \"ml\"\n",
    "    print(f\"\\n--- Starting Training: {lang_tag.upper()} ({epochs} epochs) ---\")\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(training_data.reset_index(drop=True))\n",
    "    test_dataset = Dataset.from_pandas(test_data.reset_index(drop=True))\n",
    "\n",
    "    num_proc = os.cpu_count() or 1\n",
    "    tokenized_train = train_dataset.map(\n",
    "        preprocess_function, batched=True, \n",
    "        remove_columns=train_dataset.column_names, \n",
    "        num_proc=num_proc\n",
    "        )\n",
    "    tokenized_test = test_dataset.map(\n",
    "        preprocess_function, batched=True, \n",
    "        remove_columns=test_dataset.column_names, \n",
    "        num_proc=num_proc\n",
    "        )\n",
    "    \n",
    "    output_dir_path = os.path.join(TRAIN_ARGS_ROOT_DIR, f\"results_lora_{lang_tag}\")\n",
    "    os.makedirs(output_dir_path, exist_ok=True)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir_path,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=epochs,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        report_to=\"none\",\n",
    "        seed=RANDOM_STATE,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    save_dir = os.path.join(SAVE_ROOT_DIR, f\"lora_{lang_tag}\")\n",
    "    peft_model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    \n",
    "    print(f\"Adapter for {lang_tag.upper()} saved to {save_dir}\")\n",
    "\n",
    "    device = peft_model.device\n",
    "    base_model_eval = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        config=BASE_MODEL_CONFIG\n",
    "        ).to(device)\n",
    "    final_model = PeftModel.from_pretrained(\n",
    "        base_model_eval, \n",
    "        save_dir\n",
    "        )\n",
    "\n",
    "    preds, golds = predict_emotions_llm(final_model, tokenizer, test_data, lang_tag)\n",
    "    final_metrics = compute_metrics_numeric(preds, golds)\n",
    "    \n",
    "    final_metrics[\"language\"] = lang_tag\n",
    "    print(f\"Evaluation Metrics for {lang_tag.upper()}:\")\n",
    "    for k, v in final_metrics.items():\n",
    "        if k != 'language':\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "            \n",
    "    del base_model_eval, final_model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_metrics\n",
    "\n",
    "# --- Cross-Validation Function ---\n",
    "\n",
    "def run_cross_validation(\n",
    "        lang: str, \n",
    "        tokenizer: AutoTokenizer, \n",
    "        grid: Dict[str, List[int]], \n",
    "        device: torch.device\n",
    "        ) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Performs a grid search over LoRA hyperparameters on a single language split \n",
    "    and returns the best configuration based on micro_f1.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Starting LoRA Cross-Validation on {lang.upper()} dataset ===\")\n",
    "    \n",
    "    # Load and tokenize CV data once\n",
    "    train_df, test_df = load_data(lang)\n",
    "    train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "    test_df_subset = test_df.head(MAX_SAMPLES_EVAL).reset_index(drop=True)\n",
    "    \n",
    "    num_proc = os.cpu_count() or 1\n",
    "    tokenized_train = train_dataset.map(\n",
    "        preprocess_function, \n",
    "        batched=True, \n",
    "        remove_columns=train_dataset.column_names, \n",
    "        num_proc=num_proc\n",
    "        )\n",
    "    \n",
    "    best_f1 = -1.0\n",
    "    best_params = {}\n",
    "    \n",
    "    for r_val in grid[\"r\"]:\n",
    "        for alpha_val in grid[\"lora_alpha\"]:\n",
    "            print(f\"\\n--- Testing R={r_val}, Alpha={alpha_val} ---\")\n",
    "            \n",
    "            cv_lora_config = LoraConfig(\n",
    "                r=r_val, lora_alpha=alpha_val, target_modules=[\"q\", \"v\"],\n",
    "                lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_2_SEQ_LM\"\n",
    "            )\n",
    "            cv_model = get_peft_model(\n",
    "                AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    MODEL_NAME, config=BASE_MODEL_CONFIG\n",
    "                    ).to(device), \n",
    "                    cv_lora_config\n",
    "                    )\n",
    "\n",
    "            cv_args = TrainingArguments(\n",
    "                output_dir=CV_RESULTS_DIR, per_device_train_batch_size=BATCH_SIZE,\n",
    "                per_device_eval_batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE,\n",
    "                num_train_epochs=CV_EPOCHS, evaluation_strategy=\"no\", save_strategy=\"no\",\n",
    "                logging_dir=\"./cv_logs\", report_to=\"none\", seed=RANDOM_STATE,\n",
    "            )\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer, model=cv_model)\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=cv_model, args=cv_args, train_dataset=tokenized_train,\n",
    "                tokenizer=tokenizer, data_collator=data_collator\n",
    "            )\n",
    "            \n",
    "            trainer.train()\n",
    "            temp_save_dir = os.path.join(CV_WEIGHTS_DIR, f\"lora_{r_val}_{alpha_val}\")\n",
    "            cv_model.save_pretrained(temp_save_dir)\n",
    "            \n",
    "            eval_base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                MODEL_NAME, \n",
    "                config=BASE_MODEL_CONFIG\n",
    "                ).to(device)\n",
    "            eval_model = PeftModel.from_pretrained(eval_base, temp_save_dir)\n",
    "            \n",
    "            preds, golds = predict_emotions_llm(\n",
    "                eval_model, \n",
    "                tokenizer, \n",
    "                test_df_subset, lang)\n",
    "            metrics = compute_metrics_numeric(preds, golds)\n",
    "            \n",
    "            micro_f1 = metrics[\"micro_f1\"]\n",
    "            print(f\"Micro F1: {micro_f1:.4f}\")\n",
    "\n",
    "            if micro_f1 > best_f1:\n",
    "                best_f1 = micro_f1\n",
    "                best_params = {\"r\": r_val, \"lora_alpha\": alpha_val}\n",
    "            \n",
    "            del cv_model, trainer, eval_base, eval_model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "    print(\"==============================================\")\n",
    "    print(f\"Best LoRA Hyperparameters (Micro F1={best_f1:.4f}):\")\n",
    "    print(best_params)\n",
    "    print(\"==============================================\")\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "# --- Main Orchestration Function ---\n",
    "\n",
    "def main():\n",
    "    os.makedirs(SAVE_ROOT_DIR, exist_ok=True)\n",
    "    os.makedirs(TRAIN_ARGS_ROOT_DIR, exist_ok=True)\n",
    "    os.makedirs(CV_RESULTS_DIR, exist_ok=True)\n",
    "    os.makedirs(CV_WEIGHTS_DIR, exist_ok=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    best_lora_params = run_cross_validation(CV_LANG, tokenizer, LORA_GRID, device)\n",
    "    \n",
    "    final_lora_config = LoraConfig(\n",
    "        r=best_lora_params.get(\"r\", 16),\n",
    "        lora_alpha=best_lora_params.get(\"lora_alpha\", 32),\n",
    "        target_modules=[\"q\", \"v\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_2_SEQ_LM\"\n",
    "    )\n",
    "    \n",
    "    all_results: List[Dict[str, Any]] = []\n",
    "    \n",
    "    languages_to_train = LANGUAGES + [\"multilingual\"]\n",
    "    \n",
    "    for lang in languages_to_train:\n",
    "        train_lang = lang\n",
    "        \n",
    "        train_df, test_df = load_data(train_lang)\n",
    "        \n",
    "        final_model = get_peft_model(\n",
    "            AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, config=BASE_MODEL_CONFIG).to(device),\n",
    "            final_lora_config\n",
    "        )\n",
    "        \n",
    "        metrics = run_peft_training_and_evaluation(\n",
    "            train_lang, \n",
    "            final_model, \n",
    "            tokenizer, \n",
    "            train_df, \n",
    "            test_df, \n",
    "            epochs=FINAL_EPOCHS,\n",
    "            is_multilingual=(lang == \"multilingual\")\n",
    "        )\n",
    "        all_results.append(metrics)\n",
    "        \n",
    "        del final_model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(RESULTS_FILE, index=False)\n",
    "    print(f\"\\nFinal combined metrics saved to {RESULTS_FILE}\")\n",
    "    print(\"\\n--- Summary of All Final Results ---\")\n",
    "    print(results_df.to_markdown(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0a720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
