{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from itertools import product\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from sklearn.metrics import f1_score, hamming_loss, jaccard_score\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "BASE_PATH = \"XED/processed\"\n",
    "LANGUAGE = \"en\"\n",
    "MODEL_NAME = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "R_VALUE = 8\n",
    "ALPHA_VALUE = 32\n",
    "DROPOUT_VALUE = 0.10\n",
    "FINAL_EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "ALL_LABELS = [str(i) for i in range(1, 9)]\n",
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_NEW_TOKENS = 10\n",
    "EVAL_SAMPLES_COUNT_CV = 300\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"]\n",
    "\n",
    "OUTPUT_DIR_FINAL = \"./en_gpt_results/final_lora_gpt_neo_best\"\n",
    "LOG_DIR_FINAL = \"./logs_gpt_neo_en_final\"\n",
    "SAVE_DIR_FINAL = \"./weights/gpt_neo/lora_gpt_neo_en_best_final\"\n",
    "\n",
    "\n",
    "def predict_emotions_gpt2(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    df: pd.DataFrame,\n",
    "    max_samples: Optional[int] = None\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    preds: List[str] = []\n",
    "    golds: List[str] = []\n",
    "\n",
    "    df_to_process = df.head(max_samples) if max_samples is not None else df\n",
    "    total_samples = len(df_to_process)\n",
    "\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "\n",
    "    for _, row in tqdm(\n",
    "        df_to_process.iterrows(),\n",
    "        total=total_samples,\n",
    "        desc=\"Generating Predictions\"\n",
    "    ):\n",
    "        prompt = f\"Classify the emotion in this sentence: {row['text']}\\nEmotion:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        pred_text = tokenizer.decode(\n",
    "            output[0][input_length:], skip_special_tokens=True\n",
    "        ).strip().lower()\n",
    "\n",
    "        preds.append(pred_text)\n",
    "        golds.append(str(row[\"labels\"]))\n",
    "\n",
    "    return preds, golds\n",
    "\n",
    "\n",
    "def compute_metrics_numeric(\n",
    "    preds: List[str], golds: List[str]\n",
    ") -> Dict[str, float]:\n",
    "    y_true = np.zeros((len(golds), len(ALL_LABELS)))\n",
    "    y_pred = np.zeros((len(golds), len(ALL_LABELS)))\n",
    "\n",
    "    for i, (g, p) in enumerate(zip(golds, preds)):\n",
    "        true_ids = [s.strip() for s in str(g).split(\",\") if s.strip().isdigit()]\n",
    "        pred_ids = re.findall(r'\\b[1-8]\\b', str(p))\n",
    "\n",
    "        for t in true_ids:\n",
    "            if t in ALL_LABELS:\n",
    "                y_true[i, ALL_LABELS.index(t)] = 1\n",
    "        for t in pred_ids:\n",
    "            if t in ALL_LABELS:\n",
    "                y_pred[i, ALL_LABELS.index(t)] = 1\n",
    "\n",
    "    metrics = {\n",
    "        \"micro_f1\": f1_score(\n",
    "            y_true, y_pred, average=\"micro\", zero_division=0\n",
    "        ),\n",
    "        \"macro_f1\": f1_score(\n",
    "            y_true, y_pred, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"jaccard\": jaccard_score(\n",
    "            y_true, y_pred, average=\"samples\", zero_division=0\n",
    "        ),\n",
    "        \"hamming\": hamming_loss(y_true, y_pred),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        train_df = pd.read_csv(os.path.join(BASE_PATH, f\"train_{LANGUAGE}.csv\"))\n",
    "        test_df = pd.read_csv(os.path.join(BASE_PATH, f\"test_{LANGUAGE}.csv\"))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data files not found in {BASE_PATH}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side='left')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "    base_model.to(device)\n",
    "\n",
    "    def preprocess_function_local(examples):\n",
    "        texts = [\n",
    "            f\"Classify the emotion in this sentence: {t}\\nEmotion: {l}\"\n",
    "            for t, l in zip(examples[\"text\"], examples[\"labels\"])\n",
    "        ]\n",
    "        return tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    tokenized_train = train_dataset.map(\n",
    "        preprocess_function_local,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "    tokenized_test = test_dataset.map(\n",
    "        preprocess_function_local,\n",
    "        batched=True,\n",
    "        remove_columns=test_dataset.column_names\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    print(\"--- Running Zero-Shot Baseline Evaluation on GPT-Neo-1.3B ---\")\n",
    "    base_preds, base_golds = predict_emotions_gpt2(\n",
    "        base_model, tokenizer, test_df, max_samples=EVAL_SAMPLES_COUNT_CV\n",
    "    )\n",
    "    base_metrics = compute_metrics_numeric(base_preds, base_golds)\n",
    "    print(f\"Base Model Metrics (GPT-Neo-1.3B): {base_metrics}\")\n",
    "\n",
    "    del base_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"--- Starting LoRA Fine-Tuning with GPT-Neo-1.3B ---\")\n",
    "\n",
    "    base_model_ft = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    base_model_ft.resize_token_embeddings(len(tokenizer))\n",
    "    base_model_ft.to(device)\n",
    "    base_model_ft.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=R_VALUE,\n",
    "        lora_alpha=ALPHA_VALUE,\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        lora_dropout=DROPOUT_VALUE,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(base_model_ft, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR_FINAL,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=FINAL_EPOCHS,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=LOG_DIR_FINAL,\n",
    "        report_to=\"none\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"--- Running Final Fine-Tuned Model Evaluation ---\")\n",
    "\n",
    "    preds, golds = predict_emotions_gpt2(\n",
    "        model, tokenizer, test_df, max_samples=EVAL_SAMPLES_COUNT_CV\n",
    "    )\n",
    "    metrics = compute_metrics_numeric(preds, golds)\n",
    "    print(f\"Fine-Tuned Model Metrics (GPT-Neo-1.3B): {metrics}\")\n",
    "\n",
    "    model.save_pretrained(SAVE_DIR_FINAL)\n",
    "    tokenizer.save_pretrained(SAVE_DIR_FINAL)\n",
    "    print(f\"Final fine-tuned model saved to {SAVE_DIR_FINAL}\")\n",
    "\n",
    "    del model, base_model_ft, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
