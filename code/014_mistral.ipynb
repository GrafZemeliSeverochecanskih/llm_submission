{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f035c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import traceback\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    hamming_loss,\n",
    "    jaccard_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# --- GLOBAL CONSTANTS ---\n",
    "\n",
    "OUTPUT_DIR = \"./results_multilingual_test_mistral7b_prompt_only\"\n",
    "PRED_DIR = os.path.join(OUTPUT_DIR, \"prompt_predictions\")\n",
    "METRICS_DIR = os.path.join(OUTPUT_DIR, \"metrics\")\n",
    "ALL_METRICS_FILE = os.path.join(METRICS_DIR, \"combined_metrics_summary.csv\")\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "BASE_PATH = \"./XED/processed\"\n",
    "STRATEGIES = [\"zero\", \"few\", \"instruction\"]\n",
    "\n",
    "DATA_PATHS = {\n",
    "    \"en\": os.path.join(BASE_PATH, \"test_en.csv\"),\n",
    "    \"fr\": os.path.join(BASE_PATH, \"test_fr.csv\"),\n",
    "    \"es\": os.path.join(BASE_PATH, \"test_es.csv\"),\n",
    "    \"fi\": os.path.join(BASE_PATH, \"test_fi.csv\"),\n",
    "    \"ml\": os.path.join(BASE_PATH, \"test_multilingual.csv\"),\n",
    "}\n",
    "\n",
    "DATASETS = {\n",
    "    lang: pd.read_csv(path) for lang, path in DATA_PATHS.items()\n",
    "    if os.path.exists(path)\n",
    "}\n",
    "\n",
    "XED_EMOTION_MAPPING = {\n",
    "    1: \"Anger\", 2: \"Anticipation\", 3: \"Disgust\", 4: \"Fear\",\n",
    "    5: \"Joy\", 6: \"Sadness\", 7: \"Surprise\", 8: \"Trust\", 0: \"Neutral\"\n",
    "}\n",
    "\n",
    "# --- Multilingual Prompt Components ---\n",
    "\n",
    "BASE_INSTRUCTIONS = {\n",
    "    \"en\": (\n",
    "        \"Task: Predict the correct emotion label(s) for this text using XED \"\n",
    "        \"label IDs (1–8, and 0 for Neutral).\\n\"\n",
    "        \"Respond ONLY with comma-separated label numbers (e.g. 1, 3).\"\n",
    "    ),\n",
    "    \"fr\": (\n",
    "        \"Tâche: Prédire le(s) label(s) d'émotion correct(s) pour ce texte \"\n",
    "        \"en utilisant les ID d'étiquette XED (1-8, et 0 pour Neutre).\\n\"\n",
    "        \"Répondez UNIQUEMENT avec des numéros d'étiquette séparés par \"\n",
    "        \"des virgules (par exemple 1, 3).\"\n",
    "    ),\n",
    "    \"es\": (\n",
    "        \"Tarea: Predecir la(s) etiqueta(s) de emoción correcta(s) para este \"\n",
    "        \"texto usando los IDs de etiqueta XED (1-8, y 0 para Neutro).\\n\"\n",
    "        \"Responda SÓLO con números de etiqueta separados por comas \"\n",
    "        \"(par exemple 1, 3).\"\n",
    "    ),\n",
    "    \"fi\": (\n",
    "        \"Tehtävä: Ennusta oikea tunne-etiketti(t) tälle tekstille \"\n",
    "        \"käyttämällä XED-etiketti-ID:itä (1-8 ja 0 Neutraalille).\\n\"\n",
    "        \"Vastaa AINOASTAAN pilkulla erotetuilla etikettinumeriilla \"\n",
    "        \"(esim. 1, 3).\"\n",
    "    ),\n",
    "    \"default\": (\n",
    "        \"Task: Predict the correct emotion label(s) for this text using XED \"\n",
    "        \"label IDs (1–8, and 0 for Neutral).\\n\"\n",
    "        \"Respond ONLY with comma-separated label numbers (e.g. 1, 3).\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "FEW_SHOT_EXAMPLES = {\n",
    "    \"en\": (\n",
    "        \"Examples:\\n1. Text: I am so happy today! → 5\\n\"\n",
    "        \"2. Text: This makes me furious! → 1\\n\"\n",
    "        \"3. Text: I'm scared to go outside. → 4\\n\"\n",
    "    ),\n",
    "    \"fr\": (\n",
    "        \"Exemples:\\n1. Texte: Je suis si heureux aujourd'hui ! → 5\\n\"\n",
    "        \"2. Texte: Cela me rend furieux ! → 1\\n\"\n",
    "        \"3. Texte: J'ai peur de sortir. → 4\\n\"\n",
    "    ),\n",
    "    \"es\": (\n",
    "        \"Ejemplos:\\n1. Texto: ¡Estoy tan feliz hoy! → 5\\n\"\n",
    "        \"2. Texto: ¡Esto me pone furioso! → 1\\n\"\n",
    "        \"3. Texto: Tengo miedo de salir. → 4\\n\"\n",
    "    ),\n",
    "    \"fi\": (\n",
    "        \"Esimerkkejä:\\n1. Teksti: Olen niin onnellinen tänään! → 5\\n\"\n",
    "        \"2. Teksti: Tämä saa minut raivostumaan! → 1\\n\"\n",
    "        \"3. Teksti: Pelkään mennä ulos. → 4\\n\"\n",
    "    ),\n",
    "    \"default\": (\n",
    "        \"Examples:\\n1. Text: I am so happy today! → 5\\n\"\n",
    "        \"2. Text: This makes me furious! → 1\\n\"\n",
    "        \"3. Text: I'm scared to go outside. → 4\\n\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "INSTRUCTION_INTRO = {\n",
    "    \"en\": (\n",
    "        \"You are an expert emotion classifier. Use the following XED label ID \"\n",
    "        \"to Emotion Name mapping:\"\n",
    "    ),\n",
    "    \"fr\": (\n",
    "        \"Vous êtes un classificateur d'émotions expert. Utilisez la \"\n",
    "        \"correspondance suivante entre l'ID d'étiquette XED et le nom \"\n",
    "        \"de l'émotion :\"\n",
    "    ),\n",
    "    \"es\": (\n",
    "        \"Usted es un clasificador de emociones experto. Utilice el \"\n",
    "        \"siguiente mapeo de ID de etiqueta XED a Nombre de Emoción:\"\n",
    "    ),\n",
    "    \"fi\": (\n",
    "        \"Olet asiantunteva tunneluokittelija. Käytä seuraavaa \"\n",
    "        \"XED-etiketti-ID:n ja tunnenimen vastaavuutta:\"\n",
    "    ),\n",
    "    \"default\": (\n",
    "        \"You are an expert emotion classifier. Use the following XED label ID \"\n",
    "        \"to Emotion Name mapping:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "INSTRUCTION_NOTE = {\n",
    "    \"en\": (\n",
    "        \"NOTE: The required output is the XED ID (0-8), NOT the emotion name. \"\n",
    "        \"For example, 'Anger' should be '1'. Do not output ID 8 for 'Trust' \"\n",
    "        \"as 0, as this BERT-specific re-arrangement is NOT required here.\"\n",
    "    ),\n",
    "    \"fr\": (\n",
    "        \"NOTE: La sortie requise est l'ID XED (0-8), PAS le nom de \"\n",
    "        \"l'émotion. Par exemple, 'Colère' doit être '1'. Ne pas sortir \"\n",
    "        \"l'ID 8 pour 'Confiance' comme 0, car ce réarrangement spécifique \"\n",
    "        \"à BERT N'EST PAS requis ici.\"\n",
    "    ),\n",
    "    \"es\": (\n",
    "        \"NOTA: La salida requerida es l'ID XED (0-8), NO el nombre de \"\n",
    "        \"l'émotion. Por ejemplo, 'Ira' debe ser '1'. No emita el ID 8 para \"\n",
    "        \"'Confianza' como 0, ya que este rearreglo específico de BERT NO es \"\n",
    "        \"requerido aquí.\"\n",
    "    ),\n",
    "    \"fi\": (\n",
    "        \"HUOMAUTUS: Vaadittu tuloste on XED ID (0-8), EI tunnenimi. \"\n",
    "        \"Esimerkiksi 'Viha' tulee olla '1'. Älä tulosta ID 8:aa 'Luottamus' \"\n",
    "        \"kohdalla 0:na, koska tätä BERT-spesifistä uudelleenjärjestelyä EI \"\n",
    "        \"vaadita tässä.\"\n",
    "    ),\n",
    "    \"default\": (\n",
    "        \"NOTE: The required output is the XED ID (0-8), NOT the emotion name. \"\n",
    "        \"For example, 'Anger' should be '1'. Do not output ID 8 for 'Trust' \"\n",
    "        \"as 0, as this BERT-specific re-arrangement is NOT required here.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "SENTENCE_WRAPPER = {\n",
    "    \"en\": \"Given this {lang} sentence:\",\n",
    "    \"fr\": \"Étant donné cette phrase {lang}:\",\n",
    "    \"es\": \"Dada esta oración {lang}:\",\n",
    "    \"fi\": \"Tämä {lang} lause huomioiden:\",\n",
    "    \"default\": \"Given this {lang} sentence:\",\n",
    "}\n",
    "\n",
    "\n",
    "def parse_label_column(label_series: pd.Series) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Parses a pandas Series of comma-separated string labels into a list of\n",
    "    lists of integers, handling NaN/empty values.\n",
    "    \"\"\"\n",
    "    parsed_labels = []\n",
    "    for val in label_series:\n",
    "        if pd.isna(val):\n",
    "            parsed_labels.append([])\n",
    "        else:\n",
    "            labels = [\n",
    "                int(x.strip()) for x in str(val).split(\",\")\n",
    "                if x.strip().isdigit()\n",
    "            ]\n",
    "            parsed_labels.append(labels)\n",
    "    return parsed_labels\n",
    "\n",
    "\n",
    "def build_prompt(text: str, lang: str, strategy: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the input prompt for the LLM based on the text, language,\n",
    "    and prompting strategy (zero/few/instruction), using Mistral format.\n",
    "    \"\"\"\n",
    "    lang_key = lang if lang in BASE_INSTRUCTIONS else \"default\"\n",
    "    system_instruction = BASE_INSTRUCTIONS[lang_key]\n",
    "\n",
    "    if strategy == \"zero\":\n",
    "        user_query = f\"Text ({lang}): {text}\"\n",
    "\n",
    "    elif strategy == \"few\":\n",
    "        examples = FEW_SHOT_EXAMPLES[lang_key]\n",
    "        user_query = f\"{examples}\\nText ({lang}): {text}\"\n",
    "\n",
    "    elif strategy == \"instruction\":\n",
    "        sorted_keys = sorted(XED_EMOTION_MAPPING.keys())\n",
    "        emotion_list = \"\\n\".join(\n",
    "            [f\"ID {k}: {XED_EMOTION_MAPPING[k]}\" for k in sorted_keys]\n",
    "        )\n",
    "        instruction_with_map = (\n",
    "            f\"{INSTRUCTION_INTRO[lang_key]}\\n\"\n",
    "            f\"{emotion_list}\\n\\n\"\n",
    "            f\"{INSTRUCTION_NOTE[lang_key]}\"\n",
    "        )\n",
    "        sentence_wrapper = SENTENCE_WRAPPER[lang_key].format(lang=lang)\n",
    "        user_query = (\n",
    "            f\"{instruction_with_map}\\n\\n\"\n",
    "            f\"{sentence_wrapper}\\n\"\n",
    "            f'\"{text}\"'\n",
    "        )\n",
    "    else:\n",
    "        user_query = text\n",
    "\n",
    "    return f\"[INST] {system_instruction}\\n\\n{user_query} [/INST]\"\n",
    "\n",
    "\n",
    "def parse_model_output(output: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Extracts comma-separated XED ID integers from the model's raw text output.\n",
    "    \"\"\"\n",
    "    numbers = re.findall(r\"\\d+\", output)\n",
    "    return [int(n) for n in numbers if 0 <= int(n) <= 8] if numbers else []\n",
    "\n",
    "\n",
    "def compute_multilabel_metrics(\n",
    "    preds: List[List[int]], golds: List[List[int]], lang: str, strategy: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Computes a comprehensive set of multi-label classification metrics.\n",
    "    \"\"\"\n",
    "    all_labels = sorted(\n",
    "        {l for sublist in (preds + golds) for l in sublist if 0 <= l <= 8}\n",
    "    )\n",
    "    n_labels = len(all_labels)\n",
    "    label_to_idx = {lbl: i for i, lbl in enumerate(all_labels)}\n",
    "\n",
    "    if n_labels == 0:\n",
    "        return {\n",
    "            \"language\": lang, \"strategy\": strategy, \"accuracy\": 0.0,\n",
    "            \"precision_macro\": 0.0, \"recall_macro\": 0.0, \"f1_macro\": 0.0,\n",
    "            \"f1_micro\": 0.0, \"jaccard\": 0.0, \"hamming\": 1.0\n",
    "        }\n",
    "\n",
    "    def to_indicator_matrix(label_lists: List[List[int]]) -> np.ndarray:\n",
    "        \"\"\"Converts lists of labels to a binary indicator matrix.\"\"\"\n",
    "        mat = np.zeros((len(label_lists), n_labels), dtype=int)\n",
    "        for i, labs in enumerate(label_lists):\n",
    "            if isinstance(labs, (list, tuple)):\n",
    "                for l in labs:\n",
    "                    if l in label_to_idx:\n",
    "                        mat[i, label_to_idx[l]] = 1\n",
    "        return mat\n",
    "\n",
    "    y_true = to_indicator_matrix(golds)\n",
    "    y_pred = to_indicator_matrix(preds)\n",
    "\n",
    "    metrics = {\n",
    "        \"language\": lang,\n",
    "        \"strategy\": strategy,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision_macro\": precision_score(\n",
    "            y_true, y_pred, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall_macro\": recall_score(\n",
    "            y_true, y_pred, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"f1_macro\": f1_score(\n",
    "            y_true, y_pred, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"f1_micro\": f1_score(\n",
    "            y_true, y_pred, average=\"micro\", zero_division=0\n",
    "        ),\n",
    "        \"jaccard\": jaccard_score(\n",
    "            y_true, y_pred, average=\"samples\", zero_division=0\n",
    "        ),\n",
    "        \"hamming\": hamming_loss(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def load_metrics_summary() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the existing metrics summary file or creates an empty DataFrame.\n",
    "    \"\"\"\n",
    "    if os.path.exists(ALL_METRICS_FILE):\n",
    "        try:\n",
    "            return pd.read_csv(ALL_METRICS_FILE)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Metrics file {ALL_METRICS_FILE} is empty. Starting fresh.\")\n",
    "    return pd.DataFrame(columns=[\n",
    "        \"language\", \"strategy\", \"accuracy\", \"precision_macro\",\n",
    "        \"recall_macro\", \"f1_macro\", \"f1_micro\", \"jaccard\", \"hamming\"\n",
    "    ])\n",
    "\n",
    "\n",
    "def save_metrics_summary(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Saves the updated metrics summary to the designated CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "    df.to_csv(ALL_METRICS_FILE, index=False)\n",
    "    print(f\"\\nUpdated metric summary saved to {ALL_METRICS_FILE}\")\n",
    "\n",
    "\n",
    "def run_all_prompt_tests(\n",
    "    pipe, all_datasets: Dict[str, pd.DataFrame], strategies: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs evaluation for all defined prompt strategies (zero, few, instruction)\n",
    "    on all datasets using the provided text-generation pipeline.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STARTING PROMPT ENGINEERING EVALUATION (MISTRAL-7B) ===\")\n",
    "\n",
    "    metrics_df = load_metrics_summary()\n",
    "    batch_size = 4\n",
    "\n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n--- Strategy: {strategy.upper()} ---\")\n",
    "\n",
    "        for lang, df in all_datasets.items():\n",
    "            lang_strategy_filter = (metrics_df[\"language\"] == lang) & (\n",
    "                metrics_df[\"strategy\"] == strategy)\n",
    "\n",
    "            if not metrics_df[lang_strategy_filter].empty:\n",
    "                print(f\"Skipping {lang}-{strategy}: Metrics already exist.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing {lang}-{strategy}...\")\n",
    "\n",
    "            try:\n",
    "                golds = parse_label_column(df[\"labels\"])\n",
    "                texts = df[\"text\"].tolist()\n",
    "\n",
    "                texts_subset = texts\n",
    "                golds_subset = golds\n",
    "                preds = []\n",
    "\n",
    "                for i in tqdm(\n",
    "                    range(0, len(texts_subset), batch_size),\n",
    "                    desc=f\"{lang}-{strategy}\"\n",
    "                ):\n",
    "                    batch_texts = texts_subset[i:i + batch_size]\n",
    "                    prompts = [\n",
    "                        build_prompt(t, lang, strategy) for t in batch_texts\n",
    "                    ]\n",
    "\n",
    "                    outputs = pipe(\n",
    "                        prompts,\n",
    "                        max_new_tokens=16,\n",
    "                        temperature=0.2,\n",
    "                        batch_size=batch_size,\n",
    "                        pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "                        return_full_text=False,\n",
    "                    )\n",
    "\n",
    "                    for r in outputs:\n",
    "                        if not r or not r[0] or 'generated_text' not in r[0]:\n",
    "                            text_out = \"\"\n",
    "                        else:\n",
    "                            text_out = r[0][\"generated_text\"]\n",
    "\n",
    "                        preds.append(parse_model_output(text_out))\n",
    "\n",
    "                if len(preds) != len(golds_subset):\n",
    "                    print(\n",
    "                        f\"[WARN] Pred length mismatch for {lang}-{strategy}. \"\n",
    "                        f\"Expected {len(golds_subset)}, got {len(preds)}. \"\n",
    "                        \"Filling missing predictions with empty lists.\"\n",
    "                    )\n",
    "                    preds.extend([[]] * (len(golds_subset) - len(preds)))\n",
    "\n",
    "                metrics = compute_multilabel_metrics(\n",
    "                    preds, golds_subset, lang, strategy\n",
    "                )\n",
    "                print(f\"Metrics: {metrics}\")\n",
    "\n",
    "                os.makedirs(PRED_DIR, exist_ok=True)\n",
    "                pred_file = os.path.join(\n",
    "                    PRED_DIR, f\"preds_{lang}_{strategy}.csv\"\n",
    "                )\n",
    "\n",
    "                new_preds_df = pd.DataFrame({\n",
    "                    \"pred\": [\",\".join(map(str, p)) for p in preds],\n",
    "                    \"gold\": [\",\".join(map(str, g)) for g in golds_subset],\n",
    "                    \"text\": texts_subset\n",
    "                })\n",
    "                new_preds_df.to_csv(pred_file, index=False)\n",
    "                print(f\"Predictions saved: {pred_file}\")\n",
    "\n",
    "                new_row_df = pd.DataFrame([metrics])\n",
    "                metrics_df = pd.concat(\n",
    "                    [metrics_df, new_row_df], ignore_index=True\n",
    "                )\n",
    "                save_metrics_summary(metrics_df)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"\\n[CRITICAL ERROR] Failed to process \"\n",
    "                    f\"{lang}-{strategy}: {e}\"\n",
    "                )\n",
    "                traceback.print_exc()\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution block to set up model, pipeline, and run prompt tests.\"\"\"\n",
    "    os.makedirs(PRED_DIR, exist_ok=True)\n",
    "    os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "    print(f\"Loading model: {MODEL_NAME}\")\n",
    "    global tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side='left')\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    run_all_prompt_tests(pipe, DATASETS, STRATEGIES)\n",
    "\n",
    "    del model, tokenizer, pipe\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\nExperiment stages complete and resources cleaned up.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
