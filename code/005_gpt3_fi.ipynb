{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adad98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from sklearn.metrics import f1_score, jaccard_score, hamming_loss\n",
    "\n",
    "BASE_PATH = \"XED/processed\"\n",
    "OUTPUT_ROOT_DIR = \"./fi_gpt_results\"\n",
    "LOG_ROOT_DIR = \"./logs_finnish_gpt3\"\n",
    "WEIGHTS_ROOT_DIR = \"./weights/fi_gpt_results\"\n",
    "FINAL_SAVE_DIR = os.path.join(WEIGHTS_ROOT_DIR, \"lora_gpt3_fi_best_final\")\n",
    "RESULTS_FILE = \"./results_lora_finnish_gpt3_experiments.csv\"\n",
    "\n",
    "MODEL_NAME = \"TurkuNLP/gpt3-finnish-medium\"\n",
    "LANGUAGE = \"fi\"\n",
    "ALL_LABELS = [str(i) for i in range(1, 9)]\n",
    "FINNISH_PROMPT_TEMPLATE_TRAIN = \"Luokittele t채m채n suomenkielisen lauseen tunne: {text}\\nTunne: {label}\"\n",
    "FINNISH_PROMPT_TEMPLATE_PRED = \"Luokittele t채m채n suomenkielisen lauseen tunne: {text}\\nTunne:\"\n",
    "\n",
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_NEW_TOKENS = 10\n",
    "EVAL_SAMPLES_COUNT_CV = 300 \n",
    "FINAL_EPOCHS = 5\n",
    "CV_EPOCHS = 2\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "R_VALUES = [4, 8]\n",
    "ALPHA_VALUES = [16, 32]\n",
    "DROPOUT_VALUES = [0.05, 0.1]\n",
    "TARGET_MODULES = [\"self_attention.query_key_value\", \"self_attention.dense\"]\n",
    "\n",
    "\n",
    "def load_data(lang: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Loads the training and testing datasets for the specified language.\"\"\"\n",
    "    try:\n",
    "        train_df = pd.read_csv(os.path.join(BASE_PATH, f\"train_{lang}.csv\"))\n",
    "        test_df = pd.read_csv(os.path.join(BASE_PATH, f\"test_{lang}.csv\"))\n",
    "        return train_df, test_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data files not found in {BASE_PATH}. Please check path.\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def preprocess_function(examples: Dict[str, List[Any]], tokenizer: AutoTokenizer) -> Dict[str, Any]:\n",
    "    \"\"\"Creates tokenized training samples with the Finnish prompt.\"\"\"\n",
    "    texts = [\n",
    "        FINNISH_PROMPT_TEMPLATE_TRAIN.format(text=t, label=l)\n",
    "        for t, l in zip(examples[\"text\"], examples[\"labels\"])\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "\n",
    "def setup_model_and_datasets(model_name: str, train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, Dataset, Dataset]:\n",
    "    \"\"\"Loads model, tokenizer, and prepares tokenized datasets.\"\"\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    num_proc = os.cpu_count() or 1\n",
    "    \n",
    "    preprocess_func = lambda x: preprocess_function(x, tokenizer)\n",
    "    \n",
    "    tokenized_train = Dataset.from_pandas(train_df).map(\n",
    "        preprocess_func, batched=True, remove_columns=train_df.columns.tolist(), num_proc=num_proc\n",
    "    )\n",
    "    tokenized_test = Dataset.from_pandas(test_df).map(\n",
    "        preprocess_func, batched=True, remove_columns=test_df.columns.tolist(), num_proc=num_proc\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    return base_model, tokenizer, data_collator, tokenized_train, tokenized_test\n",
    "\n",
    "\n",
    "def predict_emotions_gpt2(\n",
    "    model: PeftModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    df: pd.DataFrame,\n",
    "    max_samples: Optional[int] = None\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Generate predictions for emotion classification using the Finnish prompt.\"\"\"\n",
    "\n",
    "    preds: List[str] = []\n",
    "    golds: List[str] = []\n",
    "\n",
    "    df_to_process = df if max_samples is None else df.head(max_samples)\n",
    "    total_samples = len(df_to_process)\n",
    "\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "\n",
    "    for _, row in tqdm(df_to_process.iterrows(), total=total_samples, desc=\"Generating Predictions\"):\n",
    "        prompt = FINNISH_PROMPT_TEMPLATE_PRED.format(text=row['text'])\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False,\n",
    "                temperature=1.0\n",
    "            )\n",
    "\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        pred_text = tokenizer.decode(output[0][input_length:], skip_special_tokens=True).lower().strip()\n",
    "\n",
    "        preds.append(pred_text)\n",
    "        golds.append(str(row[\"labels\"]))\n",
    "\n",
    "    return preds, golds\n",
    "\n",
    "\n",
    "def compute_metrics_numeric(preds: List[str], golds: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Computes multi-label classification metrics (micro/macro F1, Jaccard, Hamming).\"\"\"\n",
    "\n",
    "    y_true = np.zeros((len(golds), len(ALL_LABELS)))\n",
    "    y_pred = np.zeros((len(golds), len(ALL_LABELS)))\n",
    "\n",
    "    for i, (g, p) in enumerate(zip(golds, preds)):\n",
    "        true_ids = [s.strip() for s in str(g).split(\",\") if s.strip().isdigit()]\n",
    "        pred_ids = [s.strip() for s in re.findall(r'\\b\\d\\b', str(p)) if s.strip().isdigit()]\n",
    "\n",
    "        for t in true_ids:\n",
    "            if t in ALL_LABELS:\n",
    "                y_true[i, ALL_LABELS.index(t)] = 1\n",
    "        for t in pred_ids:\n",
    "            if t in ALL_LABELS:\n",
    "                y_pred[i, ALL_LABELS.index(t)] = 1\n",
    "\n",
    "    metrics = {\n",
    "        \"micro_f1\": f1_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"jaccard\": jaccard_score(y_true, y_pred, average=\"samples\", zero_division=0),\n",
    "        \"hamming\": hamming_loss(y_true, y_pred),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION BLOCKS ---\n",
    "\n",
    "def run_lora_grid_search(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    data_collator: DataCollatorForLanguageModeling,\n",
    "    tokenized_train: Dataset,\n",
    "    tokenized_test: Dataset,\n",
    "    test_df: pd.DataFrame,\n",
    "    param_grid: List[Dict[str, float]]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Runs all grid search combinations and saves results.\n",
    "    Returns the parameters of the best performing model based on micro_f1.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"1) STARTING LORA HYPERPARAMETER GRID SEARCH ({len(param_grid)} runs)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    grid_results = []\n",
    "    best_f1 = -1.0\n",
    "    best_params = {}\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for params in param_grid:\n",
    "        r_val, alpha_val, dropout_val = params[\"r\"], params[\"lora_alpha\"], params[\"lora_dropout\"]\n",
    "        print(f\"\\n--- Training with LoRA params: r={r_val}, alpha={alpha_val}, dropout={dropout_val} ---\")\n",
    "\n",
    "        base_model_run = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "        base_model_run.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=r_val, lora_alpha=alpha_val, target_modules=TARGET_MODULES,\n",
    "            lora_dropout=dropout_val, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        model = get_peft_model(base_model_run, lora_config)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=os.path.join(OUTPUT_ROOT_DIR, f\"results_lora_finnish_gpt3_r{r_val}_a{alpha_val}\"),\n",
    "            per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE, num_train_epochs=CV_EPOCHS,\n",
    "            eval_strategy=\"epoch\", save_strategy=\"no\", logging_dir=LOG_ROOT_DIR,\n",
    "            report_to=\"none\", fp16=torch.cuda.is_available(), seed=RANDOM_STATE,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model, args=training_args, train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_test, tokenizer=tokenizer, data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        preds, golds = predict_emotions_gpt2(model, tokenizer, test_df, max_samples=EVAL_SAMPLES_COUNT_CV)\n",
    "        metrics = compute_metrics_numeric(preds, golds)\n",
    "        metrics.update(params)\n",
    "        grid_results.append(metrics)\n",
    "\n",
    "        print(f\" Results for r={r_val}, alpha={alpha_val}: {metrics}\")\n",
    "\n",
    "        save_dir = os.path.join(WEIGHTS_ROOT_DIR, f\"lora_finnish_gpt3_r{r_val}_a{alpha_val}\")\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "        if metrics['micro_f1'] > best_f1:\n",
    "            best_f1 = metrics['micro_f1']\n",
    "            best_params = params\n",
    "\n",
    "        del model, base_model_run, trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    results_df = pd.DataFrame(grid_results)\n",
    "    results_df.to_csv(RESULTS_FILE, index=False)\n",
    "\n",
    "    print(\"\\n All experiments complete. Summary:\")\n",
    "    print(results_df.sort_values(\"micro_f1\", ascending=False).to_markdown(index=False))\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def run_final_fine_tuning(\n",
    "    best_params: Dict[str, float],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    data_collator: DataCollatorForLanguageModeling,\n",
    "    tokenized_train: Dataset,\n",
    "    tokenized_test: Dataset,\n",
    "    final_epochs: int,\n",
    "    test_df: pd.DataFrame\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs final, longer fine-tuning using the best LoRA parameters.\n",
    "    Evaluates the final model on the whole test dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"2) STARTING FINAL FINE-TUNING ({final_epochs} epochs) WITH BEST PARAMS\")\n",
    "    print(f\"   Best Params: R={best_params['r']}, Alpha={best_params['lora_alpha']}, Dropout={best_params['lora_dropout']}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=best_params[\"r\"], lora_alpha=best_params[\"lora_alpha\"], target_modules=TARGET_MODULES,\n",
    "        lora_dropout=best_params[\"lora_dropout\"], bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(OUTPUT_ROOT_DIR, \"results_lora_finnish_gpt3_best_model\"),\n",
    "        per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=final_epochs,\n",
    "        eval_strategy=\"epoch\", save_strategy=\"epoch\", save_total_limit=2,\n",
    "        load_best_model_at_end=True, metric_for_best_model=\"eval_loss\", greater_is_better=False,\n",
    "        logging_dir=os.path.join(LOG_ROOT_DIR, \"final\"),\n",
    "        report_to=\"none\", fp16=torch.cuda.is_available(), seed=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=training_args, train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test, tokenizer=tokenizer, data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(FINAL_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(FINAL_SAVE_DIR)\n",
    "    print(f\"\\nFinal fine-tuned model saved to {FINAL_SAVE_DIR}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"3) EVALUATION OF BEST MODEL ON FULL TEST SET ({len(test_df)} samples)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    del model, base_model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    base_model_eval = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "    final_model = PeftModel.from_pretrained(base_model_eval, FINAL_SAVE_DIR)\n",
    "    final_model = final_model.merge_and_unload()\n",
    "    final_model.eval()\n",
    "\n",
    "    preds, golds = predict_emotions_gpt2(final_model, tokenizer, test_df, max_samples=None)\n",
    "    final_metrics = compute_metrics_numeric(preds, golds)\n",
    "\n",
    "    print(\"\\n=============================================\")\n",
    "    print(f\"FINAL EVALUATION RESULTS ({MODEL_NAME} + LoRA, {FINAL_EPOCHS} Epochs)\")\n",
    "    print(\"=============================================\")\n",
    "    for k, v in final_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    print(\"=============================================\")\n",
    "\n",
    "    del final_model, base_model_eval\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# --- MAIN ORCHESTRATOR ---\n",
    "\n",
    "def main():\n",
    "\n",
    "    os.makedirs(WEIGHTS_ROOT_DIR, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_ROOT_DIR, exist_ok=True)\n",
    "    os.makedirs(LOG_ROOT_DIR, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        train_df, test_df = load_data(LANGUAGE)\n",
    "    except FileNotFoundError:\n",
    "        return\n",
    "\n",
    "    _, tokenizer, data_collator, tokenized_train, tokenized_test = \\\n",
    "        setup_model_and_datasets(MODEL_NAME, train_df, test_df)\n",
    "\n",
    "    param_grid = [\n",
    "        {\"r\": r, \"lora_alpha\": alpha, \"lora_dropout\": drop}\n",
    "        for r, alpha, drop in product(R_VALUES, ALPHA_VALUES, DROPOUT_VALUES)\n",
    "    ]\n",
    "\n",
    "    best_params = run_lora_grid_search(\n",
    "        tokenizer, data_collator, tokenized_train, tokenized_test, test_df, param_grid\n",
    "    )\n",
    "\n",
    "    run_final_fine_tuning(\n",
    "        best_params, tokenizer, data_collator, tokenized_train, tokenized_test, FINAL_EPOCHS, test_df\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
