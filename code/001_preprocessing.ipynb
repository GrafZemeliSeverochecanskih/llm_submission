{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f87d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BASE_DIR = \"XED/\"\n",
    "ANNOTATED_PATH = os.path.join(BASE_DIR, \"AnnotatedData\")\n",
    "PROJECTIONS_PATH = os.path.join(BASE_DIR, \"Projections\")\n",
    "LANGUAGES = [\"en\", \"fi\", \"fr\", \"es\"]\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"processed\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Functions ---\n",
    "\n",
    "def load_xed_file(lang: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and clean XED data for a given language. Determines file path \n",
    "    based on whether the data is annotated (en, fi) or projected (fr, es).\n",
    "\n",
    "    Args:\n",
    "        lang: The language code (e.g., 'en', 'fr').\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame with standardized 'text', 'labels', and 'language' columns.\n",
    "    \"\"\"\n",
    "    if lang in [\"en\", \"fi\"]:\n",
    "        file_path = os.path.join(ANNOTATED_PATH, f\"{lang}-annotated.tsv\")\n",
    "    else:\n",
    "        file_path = os.path.join(PROJECTIONS_PATH, f\"{lang}-projections.tsv\")\n",
    "\n",
    "    data_frame = pd.read_csv(file_path, sep=\"\\t\")\n",
    "    \n",
    "    # Standardize column names (lowercase and strip whitespace)\n",
    "    data_frame.columns = [col.strip().lower() for col in data_frame.columns]\n",
    "    \n",
    "    text_cols = [col for col in data_frame.columns if \"text\" in col or \"utterance\" in col]\n",
    "    label_cols = [col for col in data_frame.columns if \"label\" in col or \"emotion\" in col]\n",
    "    \n",
    "    # Fallback to the first two columns if automatic detection fails\n",
    "    if not text_cols:\n",
    "        text_col_name = data_frame.columns[0]\n",
    "    else:\n",
    "        text_col_name = text_cols[0]\n",
    "        \n",
    "    if not label_cols:\n",
    "        label_col_name = data_frame.columns[1]\n",
    "    else:\n",
    "        label_col_name = label_cols[0]\n",
    "    \n",
    "    data_frame = data_frame[[text_col_name, label_col_name]]\n",
    "    data_frame.columns = [\"text\", \"labels\"]\n",
    "    \n",
    "    data_frame[\"language\"] = lang\n",
    "    return data_frame\n",
    "\n",
    "# --- Data Loading and Concatenation ---\n",
    "\n",
    "data_frames: List[pd.DataFrame] = []\n",
    "for lang_code in LANGUAGES:\n",
    "    try:\n",
    "        df = load_xed_file(lang_code)\n",
    "        print(f\"Loaded {lang_code}: {df.shape}\")\n",
    "        data_frames.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {lang_code}: {e}\")\n",
    "\n",
    "all_data_combined = pd.concat(data_frames, ignore_index=True)\n",
    "print(f\"\\nCombined shape: {all_data_combined.shape}\")\n",
    "print(\"Sample data head:\")\n",
    "print(all_data_combined.head())\n",
    "\n",
    "# --- Data Cleaning ---\n",
    "\n",
    "print(f\"\\nDuplicates before cleaning: {all_data_combined.duplicated(subset=['text']).sum()}\")\n",
    "\n",
    "# Drop duplicates based on the 'text' column\n",
    "all_data_combined = all_data_combined.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
    "print(f\"After removing duplicates: {all_data_combined.shape}\")\n",
    "\n",
    "# Drop rows with missing data in 'text' or 'labels'\n",
    "missing_labels_count = all_data_combined[\"labels\"].isna().sum()\n",
    "print(f\"Missing label rows: {missing_labels_count}\")\n",
    "all_data_combined = all_data_combined.dropna(subset=[\"text\", \"labels\"])\n",
    "print(f\"After dropping NA rows: {all_data_combined.shape}\")\n",
    "\n",
    "# --- Splitting and Saving ---\n",
    "\n",
    "train_data_frames: List[pd.DataFrame] = []\n",
    "test_data_frames: List[pd.DataFrame] = []\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "for lang_code in LANGUAGES:\n",
    "    lang_df = all_data_combined[all_data_combined[\"language\"] == lang_code]\n",
    "    \n",
    "    # Split data per language (logic preserved)\n",
    "    train_split, test_split = train_test_split(\n",
    "        lang_df, \n",
    "        test_size=TEST_SIZE, \n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    train_data_frames.append(train_split)\n",
    "    test_data_frames.append(test_split)\n",
    "    print(f\"{lang_code}: train={train_split.shape[0]}, test={test_split.shape[0]}\")\n",
    "\n",
    "print(\"\\nSaving individual language splits...\")\n",
    "for lang_code, train_df, test_df in zip(LANGUAGES, train_data_frames, test_data_frames):\n",
    "    train_df.to_csv(os.path.join(OUTPUT_DIR, f\"train_{lang_code}.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(OUTPUT_DIR, f\"test_{lang_code}.csv\"), index=False)\n",
    "\n",
    "train_multi = pd.concat(train_data_frames, ignore_index=True)\n",
    "test_multi = pd.concat(test_data_frames, ignore_index=True)\n",
    "\n",
    "train_multi.to_csv(os.path.join(OUTPUT_DIR, \"train_multilingual.csv\"), index=False)\n",
    "test_multi.to_csv(os.path.join(OUTPUT_DIR, \"test_multilingual.csv\"), index=False)\n",
    "\n",
    "print(\"\\nSaved multilingual splits:\")\n",
    "print(f\"Train: {train_multi.shape} | Test: {test_multi.shape}\")\n",
    "\n",
    "# --- Final Checks ---\n",
    "print(\"\\nUnique languages in combined training set:\", train_multi[\"language\"].unique())\n",
    "print(\"Sample labels distribution (Top 5 in training set):\")\n",
    "print(train_multi[\"labels\"].value_counts().head())\n",
    "print(f\"\\nPreprocessing complete. Files saved in: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
