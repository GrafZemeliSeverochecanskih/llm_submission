{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "from itertools import product\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from sklearn.metrics import f1_score, hamming_loss, jaccard_score\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "BASE_PATH = \"XED/processed\"\n",
    "OUTPUT_ROOT_DIR = \"./es_stablelm_results\"\n",
    "LOG_ROOT_DIR = \"./logs_stablelm_es\"\n",
    "WEIGHTS_ROOT_DIR = \"./weights/stablelm_es\"\n",
    "FINAL_SAVE_DIR = os.path.join(WEIGHTS_ROOT_DIR, \"lora_stablelm_es_best_final\")\n",
    "RESULTS_FILE = \"./results_lora_stablelm_es_experiments.csv\"\n",
    "\n",
    "MODEL_NAME = \"stabilityai/stablelm-2-zephyr-1_6b\"\n",
    "LANGUAGE = \"es\"\n",
    "ALL_LABELS = [str(i) for i in range(1, 9)]\n",
    "SPANISH_PROMPT_TEMPLATE_TRAIN = (\n",
    "    \"Clasifica la emoción en esta frase en español: {text}\\nEmoción: {label}\"\n",
    ")\n",
    "SPANISH_PROMPT_TEMPLATE_PRED = (\n",
    "    \"Clasifica la emoción en esta frase en español: {text}\\nEmoción:\"\n",
    ")\n",
    "\n",
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_NEW_TOKENS = 10\n",
    "EVAL_SAMPLES_COUNT_CV = 300\n",
    "FINAL_EPOCHS = 5\n",
    "CV_EPOCHS = 2\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "R_VALUES = [4, 8]\n",
    "ALPHA_VALUES = [16, 32]\n",
    "DROPOUT_VALUES = [0.05, 0.1]\n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"up_proj\", \"down_proj\", \"gate_proj\"\n",
    "]\n",
    "\n",
    "\n",
    "def load_data(lang: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    try:\n",
    "        train_df = pd.read_csv(os.path.join(BASE_PATH, f\"train_{lang}.csv\"))\n",
    "        test_df = pd.read_csv(os.path.join(BASE_PATH, f\"test_{lang}.csv\"))\n",
    "        return train_df, test_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data files not found in {BASE_PATH}. Please check path.\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def preprocess_function(\n",
    "    examples: Dict[str, List[Any]], tokenizer: AutoTokenizer\n",
    ") -> Dict[str, Any]:\n",
    "    texts = [\n",
    "        SPANISH_PROMPT_TEMPLATE_TRAIN.format(text=t, label=l)\n",
    "        for t, l in zip(examples[\"text\"], examples[\"labels\"])\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "\n",
    "def setup_model_and_datasets(\n",
    "    model_name: str, train_df: pd.DataFrame, test_df: pd.DataFrame\n",
    ") -> Tuple[\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Dataset,\n",
    "    Dataset\n",
    "]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        torch_dtype=(\n",
    "            torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        ),\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    base_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    base_model.config.use_cache = False\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    num_proc = os.cpu_count() or 1\n",
    "    preprocess_func = lambda x: preprocess_function(x, tokenizer)\n",
    "\n",
    "    tokenized_train = Dataset.from_pandas(train_df).map(\n",
    "        preprocess_func,\n",
    "        batched=True,\n",
    "        remove_columns=train_df.columns.tolist(),\n",
    "        num_proc=num_proc\n",
    "    )\n",
    "    tokenized_test = Dataset.from_pandas(test_df).map(\n",
    "        preprocess_func,\n",
    "        batched=True,\n",
    "        remove_columns=test_df.columns.tolist(),\n",
    "        num_proc=num_proc\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False\n",
    "    )\n",
    "\n",
    "    return base_model, tokenizer, data_collator, tokenized_train, tokenized_test\n",
    "\n",
    "\n",
    "def predict_emotions_spanish(\n",
    "    model: PeftModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    df: pd.DataFrame,\n",
    "    max_samples: Optional[int] = None\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    preds: List[str] = []\n",
    "    golds: List[str] = []\n",
    "\n",
    "    df_to_process = df if max_samples is None else df.head(max_samples)\n",
    "    total_samples = len(df_to_process)\n",
    "\n",
    "    input_device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    for _, row in tqdm(\n",
    "        df_to_process.iterrows(),\n",
    "        total=total_samples,\n",
    "        desc=\"Generating Predictions\"\n",
    "    ):\n",
    "        prompt = SPANISH_PROMPT_TEMPLATE_PRED.format(text=row['text'])\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(input_device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False,\n",
    "                temperature=1.0\n",
    "            )\n",
    "\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        pred_text = tokenizer.decode(\n",
    "            output[0][input_length:], skip_special_tokens=True\n",
    "        ).lower().strip()\n",
    "\n",
    "        preds.append(pred_text)\n",
    "        golds.append(str(row[\"labels\"]))\n",
    "\n",
    "    return preds, golds\n",
    "\n",
    "\n",
    "def compute_metrics_numeric(\n",
    "    preds: List[str], golds: List[str]\n",
    ") -> Dict[str, float]:\n",
    "    y_true = np.zeros((len(golds), len(ALL_LABELS)))\n",
    "    y_pred = np.zeros((len(golds), len(ALL_LABELS)))\n",
    "\n",
    "    for i, (g, p) in enumerate(zip(golds, preds)):\n",
    "        true_ids = [s.strip() for s in str(g).split(\",\") if s.strip().isdigit()]\n",
    "        pred_ids = [\n",
    "            s.strip() for s in re.findall(r'\\b\\d\\b', str(p)) if s.strip().isdigit()\n",
    "        ]\n",
    "\n",
    "        for t in true_ids:\n",
    "            if t in ALL_LABELS:\n",
    "                y_true[i, ALL_LABELS.index(t)] = 1\n",
    "        for t in pred_ids:\n",
    "            if t in ALL_LABELS:\n",
    "                y_pred[i, ALL_LABELS.index(t)] = 1\n",
    "\n",
    "    metrics = {\n",
    "        \"micro_f1\": f1_score(\n",
    "            y_true, y_pred, average=\"micro\", zero_division=0\n",
    "        ),\n",
    "        \"macro_f1\": f1_score(\n",
    "            y_true, y_pred, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"jaccard\": jaccard_score(\n",
    "            y_true, y_pred, average=\"samples\", zero_division=0\n",
    "        ),\n",
    "        \"hamming\": hamming_loss(y_true, y_pred),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def run_lora_grid_search(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    data_collator: DataCollatorForLanguageModeling,\n",
    "    tokenized_train: Dataset,\n",
    "    tokenized_test: Dataset,\n",
    "    test_df: pd.DataFrame,\n",
    "    param_grid: List[Dict[str, float]]\n",
    ") -> Dict[str, float]:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"1) STARTING LORA HYPERPARAMETER GRID SEARCH ({len(param_grid)} runs)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    grid_results = []\n",
    "    best_f1 = -1.0\n",
    "    best_params = {}\n",
    "\n",
    "    for params in param_grid:\n",
    "        r_val = params[\"r\"]\n",
    "        alpha_val = params[\"lora_alpha\"]\n",
    "        dropout_val = params[\"lora_dropout\"]\n",
    "        print(\n",
    "            f\"\\n=== Training with LoRA params: r={r_val}, \"\n",
    "            f\"alpha={alpha_val}, dropout={dropout_val} ===\"\n",
    "        )\n",
    "\n",
    "        base_model_run = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=(\n",
    "                torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "            ),\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        base_model_run.config.pad_token_id = tokenizer.eos_token_id\n",
    "        base_model_run.config.use_cache = False\n",
    "        base_model_run.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=r_val,\n",
    "            lora_alpha=alpha_val,\n",
    "            target_modules=TARGET_MODULES,\n",
    "            lora_dropout=dropout_val,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        model = get_peft_model(base_model_run, lora_config)\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.enable_input_require_grads()\n",
    "\n",
    "        output_dir = os.path.join(\n",
    "            OUTPUT_ROOT_DIR,\n",
    "            f\"results_lora_stablelm_r{r_val}_a{alpha_val}_d{dropout_val}\"\n",
    "        )\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            num_train_epochs=CV_EPOCHS,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            logging_dir=LOG_ROOT_DIR,\n",
    "            report_to=\"none\",\n",
    "            fp16=False,\n",
    "            bf16=torch.cuda.is_available(),\n",
    "            seed=RANDOM_STATE,\n",
    "            gradient_checkpointing=True,\n",
    "            max_grad_norm=1.0,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_test,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        preds, golds = predict_emotions_spanish(\n",
    "            model, tokenizer, test_df, max_samples=EVAL_SAMPLES_COUNT_CV\n",
    "        )\n",
    "        metrics = compute_metrics_numeric(preds, golds)\n",
    "        metrics.update(params)\n",
    "        grid_results.append(metrics)\n",
    "\n",
    "        print(\n",
    "            f\"Results for r={r_val}, alpha={alpha_val}, \"\n",
    "            f\"dropout={dropout_val}: {metrics}\"\n",
    "        )\n",
    "\n",
    "        save_dir = os.path.join(\n",
    "            WEIGHTS_ROOT_DIR, f\"lora_r{r_val}_a{alpha_val}_d{dropout_val}\"\n",
    "        )\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "        if metrics['micro_f1'] > best_f1:\n",
    "            best_f1 = metrics['micro_f1']\n",
    "            best_params = params\n",
    "\n",
    "        del model, base_model_run, trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    results_df = pd.DataFrame(grid_results)\n",
    "    results_df.to_csv(RESULTS_FILE, index=False)\n",
    "\n",
    "    print(\"\\n All experiments complete. Summary:\")\n",
    "    print(\n",
    "        results_df.sort_values(\n",
    "            \"micro_f1\", ascending=False\n",
    "        ).to_markdown(index=False)\n",
    "    )\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def run_final_fine_tuning(\n",
    "    best_params: Dict[str, float],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    data_collator: DataCollatorForLanguageModeling,\n",
    "    tokenized_train: Dataset,\n",
    "    tokenized_test: Dataset,\n",
    "    final_epochs: int,\n",
    "    test_df: pd.DataFrame\n",
    "):\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"2) STARTING FINAL FINE-TUNING ({final_epochs} epochs) WITH BEST PARAMS\")\n",
    "    if best_params:\n",
    "        print(\n",
    "            f\"   Best Params: R={best_params.get('r')}, \"\n",
    "            f\"Alpha={best_params.get('lora_alpha')}, \"\n",
    "            f\"Dropout={best_params.get('lora_dropout')}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"   Warning: No best parameters found from grid search. Using default.\")\n",
    "        best_params = {\"r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.05}\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=(\n",
    "            torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        ),\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    base_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    base_model.config.use_cache = False\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=best_params[\"r\"],\n",
    "        lora_alpha=best_params[\"lora_alpha\"],\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_dropout=best_params[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    output_dir = os.path.join(\n",
    "        OUTPUT_ROOT_DIR, \"results_lora_stablelm_best_final\"\n",
    "    )\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=final_epochs,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        logging_dir=os.path.join(LOG_ROOT_DIR, \"final\"),\n",
    "        report_to=\"none\",\n",
    "        fp16=False,\n",
    "        bf16=torch.cuda.is_available(),\n",
    "        seed=RANDOM_STATE,\n",
    "        gradient_checkpointing=True,\n",
    "        max_grad_norm=1.0,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(FINAL_SAVE_DIR)\n",
    "    tokenizer.save_pretrained(FINAL_SAVE_DIR)\n",
    "    print(f\"\\nFinal fine-tuned model saved to {FINAL_SAVE_DIR}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"3) EVALUATION OF BEST MODEL ON FULL TEST SET ({len(test_df)} samples)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    del model, base_model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    base_model_eval = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=(\n",
    "            torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        ),\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    base_model_eval.config.pad_token_id = tokenizer.eos_token_id\n",
    "    base_model_eval.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    final_model = PeftModel.from_pretrained(base_model_eval, FINAL_SAVE_DIR)\n",
    "    final_model = final_model.merge_and_unload()\n",
    "    final_model.eval()\n",
    "\n",
    "    preds, golds = predict_emotions_spanish(\n",
    "        final_model, tokenizer, test_df, max_samples=None\n",
    "    )\n",
    "    final_metrics = compute_metrics_numeric(preds, golds)\n",
    "\n",
    "    print(\"\\n=============================================\")\n",
    "    print(\n",
    "        f\"FINAL EVALUATION RESULTS ({MODEL_NAME} + LoRA, {FINAL_EPOCHS} Epochs)\"\n",
    "    )\n",
    "    print(\"=============================================\")\n",
    "    for k, v in final_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    print(\"=============================================\")\n",
    "\n",
    "    del final_model, base_model_eval\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(WEIGHTS_ROOT_DIR, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_ROOT_DIR, exist_ok=True)\n",
    "    os.makedirs(LOG_ROOT_DIR, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        train_df, test_df = load_data(LANGUAGE)\n",
    "    except FileNotFoundError:\n",
    "        return\n",
    "\n",
    "    base_model, tokenizer, data_collator, tokenized_train, tokenized_test = (\n",
    "        setup_model_and_datasets(MODEL_NAME, train_df, test_df)\n",
    "    )\n",
    "    \n",
    "    del base_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    param_grid = [\n",
    "        {\"r\": r, \"lora_alpha\": alpha, \"lora_dropout\": drop}\n",
    "        for r, alpha, drop in product(\n",
    "            R_VALUES, ALPHA_VALUES, DROPOUT_VALUES\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    best_params = run_lora_grid_search(\n",
    "        tokenizer, data_collator, tokenized_train,\n",
    "        tokenized_test, test_df, param_grid\n",
    "    )\n",
    "\n",
    "    run_final_fine_tuning(\n",
    "        best_params, tokenizer, data_collator, tokenized_train,\n",
    "        tokenized_test, FINAL_EPOCHS, test_df\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
